  %%%%%%%%%%%%%%%%%%%%%  Layers specification %%%%%%%%%%%%%%%%%%
 
 	net.layers{end+1}.properties = struct('type','input', 'sizeFm' ,[64 64 32],'numFm',1 );         %inputLayer
 	net.layers{end+1}.properties = struct('type','conv','numFm',12  , 'Activation',@Relu, 'dActivation',@dRelu,'kernel',[6 6 5], 'pooling', [2 2 1]);
 	net.layers{end+1}.properties = struct('type','conv','numFm',24 , 'Activation',@Relu, 'dActivation',@dRelu,'kernel',9, 'pooling', 4,'pad',[1,1,2], 'dropOut',0.8);
 	net.layers{end+1}.properties = struct('type','fc','numFm',128, 'Activation',@Relu, 'dActivation',@dRelu, 'dropOut',0.8);
 	net.layers{end+1}.properties = struct('type','fc','numFm',26);
  	net.layers{end+1}.properties = struct('type','output','lossFunc',@CrossEnt,'costFunc',@CrossEnt_Cost);     %classification Layer

 %%%%%%%%%%%%%%%%%%%%%  Hyper params - training %%%%%%%%%%%%%%%%%%
 
 	net.hyperParam.trainLoopCount = 100;		%on how many images to train before evaluating the network
 	net.hyperParam.testImageNum   = 219;   	% after each loop, on how many images to evaluate network performance
	net.hyperParam.ni_initial     = 0.0001;	        % ni to start training process
	net.hyperParam.ni_final       = 0.000001;	% final ni to stop the training process
	net.hyperParam.momentum       = 0.9;
    net.runInfoParam.verifyBP     = 0;       
    net.hyperParam.lambda         = 0.008;
 
	net.hyperParam.flipImage      = 1;              % randomly flip the input hor/vert before passing to the network. Improves learning in some instances
	net.hyperParam.augmentParams.medianFilt = 0;  %between 0 and one - if this value is 0.75 it will zero all 75% lower points. 0 will mean no point is changed, 1 will keep the higest point only 
     	
	net.runInfoParam.storeMaxMSENet = 1;
	net.hyperParam.noImprovementTh  = 250;
